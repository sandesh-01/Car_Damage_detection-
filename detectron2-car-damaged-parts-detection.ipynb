{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1572269,"sourceType":"datasetVersion","datasetId":929361},{"sourceId":2108524,"sourceType":"datasetVersion","datasetId":1265041}],"dockerImageVersionId":30086,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a ><img src=\"https://i.ibb.co/dP12V6R/damages.jpg\" alt=\"damages\" border=\"0\"></a>\n<a ><img src=\"https://i.ibb.co/0QhJGSd/parts.jpg\" alt=\"parts\" border=\"0\"></a>","metadata":{}},{"cell_type":"markdown","source":"The left image shows the damages and right image shows the parts, both of these can be plotted using this notebook [https://www.kaggle.com/lplenka/coco-data-visualization](https://www.kaggle.com/lplenka/coco-data-visualization)","metadata":{}},{"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'>Problem Statement</center></h3>","metadata":{}},{"cell_type":"markdown","source":"#### The challenge here is to detect that the **hood ** has damages and not other parts of car.","metadata":{}},{"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'>Detecting Damaged Parts - Idea</center></h3>","metadata":{}},{"cell_type":"markdown","source":"I couldn't think of any way where we can train a image segmentation model that can directly detect the damaged parts. So I decided to build two image segmentation models. One model to segment the damages which returns the \"damage\" polygon(s). One model to segment the parts of the car which returns the \"parts\" polygon(s). \n\nI had two approaches on how to move forward after getting the output from two models:\n*  After getting the predicted bounding boxes (polygons) from two models then I can check which damage polygons lie inside which \"part\" polygon and can detect the damaged part. \n*  After getting the predicted bounding boxes (polygons) from two models then I can check how far the damage is from different parts and return the part nearest to a damage.\n\n\nNote: There should be some way to train a single model that does both the tasks, but it can be in next versions. Feel free to suggest new approaches and implement your own approach.\n\nIn this notebook I have implemented the second approach for now. I will add the first approach soon.","metadata":{}},{"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'>Source Dataset</center></h3>","metadata":{}},{"cell_type":"markdown","source":"**Since we will train two models, first for only damages and second for only parts, you can find annotation for both in the dataset I have published here. [Coco Car Damage Dataset](https://www.kaggle.com/lplenka/coco-car-damage-detection-dataset)**","metadata":{}},{"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'>Let's begin!</center></h3>","metadata":{}},{"cell_type":"markdown","source":"##### Since I have already shown the installation steps, here I will directly start with all installations","metadata":{}},{"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'>Installation</center></h3>","metadata":{}},{"cell_type":"code","source":"# Install Pycocotools\n!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n# Install detectron 2\n!python -m pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.7/index.html","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-01T07:59:47.265622Z","iopub.execute_input":"2024-03-01T07:59:47.266069Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-2gqblteu\n  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-2gqblteu\nRequirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools==2.0) (49.6.0.post20210108)\nRequirement already satisfied: cython>=0.27.3 in /opt/conda/lib/python3.7/site-packages (from pycocotools==2.0) (0.29.22)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools==2.0) (3.4.0)\nRequirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.19.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (7.2.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\nBuilding wheels for collected packages: pycocotools\n  Building wheel for pycocotools (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0-cp37-cp37m-linux_x86_64.whl size=272379 sha256=140d3b68d6c23cd199a41f4d7bf6ab60d3828c1046d2b0a667f44d1b514900dc\n  Stored in directory: /tmp/pip-ephem-wheel-cache-rjmv9k8a/wheels/e2/6b/1d/344ac773c7495ea0b85eb228bc66daec7400a143a92d36b7b1\nSuccessfully built pycocotools\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'>Import Libraries</center></h3>","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nfrom pycocotools.coco import COCO\nimport numpy as np\nimport skimage.io as io\nimport matplotlib.pyplot as plt\nimport pylab\nimport random\npylab.rcParams['figure.figsize'] = (8.0, 10.0)# Import Libraries\n\n# For visualization\nimport os\nimport seaborn as sns\nfrom matplotlib import colors\nfrom tensorboard.backend.event_processing import event_accumulator as ea\nfrom PIL import Image\n\n# Scipy for calculating distance\nfrom scipy.spatial import distance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'>Set constant variables</center></h3>","metadata":{}},{"cell_type":"code","source":"# I am visualizing some images in the 'val/' directory\n\ndataDir='../input/coco-car-damage-detection-dataset/val'\ndataType='COCO_val_annos'\nmul_dataType='COCO_mul_val_annos'\nannFile='{}/{}.json'.format(dataDir,dataType)\nmul_annFile='{}/{}.json'.format(dataDir,mul_dataType)\nimg_dir = \"../input/coco-car-damage-detection-dataset/img\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white; border:0' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'> Initialize the COCO API</center></h3>","metadata":{}},{"cell_type":"code","source":"# initialize coco api for instance annotations\ncoco=COCO(annFile)\nmul_coco=COCO(mul_annFile)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px' > Import Libraries required for training</center></h3>","metadata":{}},{"cell_type":"code","source":"import torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert torch.__version__.startswith(\"1.7\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common libraries\nimport numpy as np\nimport os, json, cv2, random\nimport matplotlib.pyplot as plt\nimport skimage.io as io\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.utils.visualizer import ColorMode\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\n\n# Set base params\nplt.rcParams[\"figure.figsize\"] = [16,9]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To find out inconsistent CUDA versions, if there is no \"failed\" word in this output then things are fine.\n!python -m detectron2.utils.collect_env","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'>  Register Car Damage Dataset </center></h3>","metadata":{"trusted":true}},{"cell_type":"markdown","source":"#### Register Train Dataset, so that we can use its Metadata","metadata":{}},{"cell_type":"code","source":"\ndataset_dir = \"../input/coco-car-damage-detection-dataset\"\nimg_dir = \"img/\"\ntrain_dir = \"train/\"\nval_dir = \"val/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from detectron2.data.datasets import register_coco_instances\n\n# Check if the dataset is already registered before attempting to register it again\nif \"car_dataset_val\" not in DatasetCatalog:\n    register_coco_instances(\"car_dataset_val\", {}, os.path.join(dataset_dir,val_dir,\"COCO_val_annos.json\"), os.path.join(dataset_dir,img_dir))\n\n# Similarly, check for the other dataset registration\nif \"car_mul_dataset_val\" not in DatasetCatalog:\n    register_coco_instances(\"car_mul_dataset_val\", {}, os.path.join(dataset_dir,val_dir,\"COCO_mul_val_annos.json\"), os.path.join(dataset_dir,img_dir))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white; ' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'> Load trained model </center></h2>","metadata":{}},{"cell_type":"markdown","source":"#### I will load two pretained models:\n\n* Damage Segmentation model weights -  This can be easily created using this notebook [\nDetectron2 Car Damage Detection](https://www.kaggle.com/lplenka/detectron2-car-damage-detection). The model is stored in default output directory.\n\n* Parts Segmentation Model weights - This can be also created just changing the dataset from damage annotions to parts annotation in [cell 22](https://www.kaggle.com/lplenka/detectron2-car-damage-detection?scriptVersionId=52171508&cellId=37)\n","metadata":{}},{"cell_type":"markdown","source":"\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white; ' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'> Damage Detection Model </center></h2>","metadata":{}},{"cell_type":"code","source":"#get configuration\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # only has one class (damage) + 1\ncfg.MODEL.RETINANET.NUM_CLASSES = 2 # only has one class (damage) + 1\ncfg.MODEL.WEIGHTS = os.path.join(\"../input/coco-damage-detection-trained-models/damage_segmentation_model.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7 \ncfg['MODEL']['DEVICE']='cuda'#or cpu\ndamage_predictor = DefaultPredictor(cfg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white; ' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'> Parts Segmentation Model </center></h2>","metadata":{}},{"cell_type":"code","source":"cfg_mul = get_cfg()\ncfg_mul.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg_mul.MODEL.ROI_HEADS.NUM_CLASSES = 6  # only has five classes (headlamp,hood,rear_bumper,front_bumper_door) + 1\ncfg_mul.MODEL.RETINANET.NUM_CLASSES = 6 # only has five classes (headlamp,hood,rear_bumper,front_bumper_door) + 1\ncfg_mul.MODEL.WEIGHTS = os.path.join(\"../input/coco-damage-detection-trained-models/part_segmentation_model.pth\")\ncfg_mul.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7 \ncfg_mul['MODEL']['DEVICE']='cuda' #or cpu\npart_predictor = DefaultPredictor(cfg_mul)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'> Model Inference </center></h2>","metadata":{}},{"cell_type":"code","source":"def detect_damage_part(damage_dict, parts_dict):\n  \"\"\"\n  Returns the most plausible damaged part for the list of damages by checking the distance \n  between centers centers of damage_polygons and parts_polygons\n\n  Parameters\n  -------------\n   damage_dict: dict\n                Dictionary that maps damages to damage polygon centers.\n   parts_dict: dict\n                Dictionary that maps part labels to parts polygon centers.\n  Return\n  ----------\n  part_name: str\n            The most plausible damaged part name.\n  \"\"\"\n  try:\n    max_distance = 10e9\n    assert len(damage_dict)>0, \"AssertError: damage_dict should have atleast one damage\"\n    assert len(parts_dict)>0, \"AssertError: parts_dict should have atleast one part\"\n    max_distance_dict = dict(zip(damage_dict.keys(),[max_distance]*len(damage_dict)))\n    part_name = dict(zip(damage_dict.keys(),['']*len(damage_dict)))\n\n    for y in parts_dict.keys():\n        for x in damage_dict.keys():\n          dis = distance.euclidean(damage_dict[x], parts_dict[y])\n          if dis < max_distance_dict[x]:\n            part_name[x] = y.rsplit('_',1)[0]\n\n    return list(set(part_name.values()))\n  except Exception as e:\n    print(e)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndamage_class_map= {0:'damage'}\nparts_class_map={0:'headlamp',1:'rear_bumper', 2:'door', 3:'hood', 4: 'front_bumper'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize =(16,12))\nim = io.imread(\"../input/coco-car-damage-detection-dataset/val/32.jpg\")\n\n#damage inference\ndamage_outputs = damage_predictor(im)\ndamage_v = Visualizer(im[:, :, ::-1],\n                   metadata=MetadataCatalog.get(\"car_dataset_val\"), \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n)\ndamage_out = damage_v.draw_instance_predictions(damage_outputs[\"instances\"].to(\"cpu\"))\n\n#part inference\nparts_outputs = part_predictor(im)\nparts_v = Visualizer(im[:, :, ::-1],\n                   metadata=MetadataCatalog.get(\"car_mul_dataset_val\"), \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n)\nparts_out = parts_v.draw_instance_predictions(parts_outputs[\"instances\"].to(\"cpu\"))\n\n#plot\nax1.imshow(damage_out.get_image()[:, :, ::-1],)\nax2.imshow(parts_out.get_image()[:, :, ::-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'> Create damage polygons </center></h2>","metadata":{}},{"cell_type":"markdown","source":"For now allowing multiple polygons of same class label","metadata":{}},{"cell_type":"code","source":"damage_prediction_classes = [ damage_class_map[el] + \"_\" + str(indx) for indx,el in enumerate(damage_outputs[\"instances\"].pred_classes.tolist())]\ndamage_polygon_centers = damage_outputs[\"instances\"].pred_boxes.get_centers().tolist()\ndamage_dict = dict(zip(damage_prediction_classes,damage_polygon_centers))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'> Create parts polygons </center></h2>","metadata":{}},{"cell_type":"markdown","source":"For now allowing multiple polygons of same class label","metadata":{}},{"cell_type":"code","source":"\nparts_prediction_classes = [ parts_class_map[el] + \"_\" + str(indx) for indx,el in enumerate(parts_outputs[\"instances\"].pred_classes.tolist())]\nparts_polygon_centers =  parts_outputs[\"instances\"].pred_boxes.get_centers().tolist()\n\n\n\n#Remove centers which lie in beyond 800 units\nparts_polygon_centers_filtered = list(filter(lambda x: x[0] < 800 and x[1] < 800, parts_polygon_centers))\nparts_dict = dict(zip(parts_prediction_classes,parts_polygon_centers_filtered))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'> Damaged Parts </center></h2>","metadata":{}},{"cell_type":"code","source":"print(\"Damaged Parts: \",detect_damage_part(damage_dict,parts_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ouput looks correct 😃","metadata":{}},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'> Some insights from the performance of model </center></h2>","metadata":{}},{"cell_type":"markdown","source":"* Model confuses between front and rear bumper.\n* Center of polygon was mostly different from center of bbox.","metadata":{}},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; color:white;' role=\"tab\" aria-controls=\"home\"><center style='padding-top: 15px'> Conclusion </center></h2>\n","metadata":{"trusted":true}},{"cell_type":"markdown","source":"* There is definitely a lot of scope for improvement. But this notebook can be a good begining for other complex approaches.\n* Data augmentation and training on larger data can significantly improve the results.","metadata":{}},{"cell_type":"markdown","source":"### Do give this notebook an upvote if you liked my work, thanks!","metadata":{}}]}